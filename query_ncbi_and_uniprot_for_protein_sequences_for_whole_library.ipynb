{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1550 unique NCBI IDs of type ['VP', 'VT']\n",
      "Found 103 unique UniProt IDs to fetch sequences for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching NCBI FASTA: 100%|██████████| 1550/1550 [10:41<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1550 unique NCBI protein sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching UniProt FASTA:   5%|▍         | 5/103 [00:06<02:13,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniProt ID A0A2I6J2E4: No sequences found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching UniProt FASTA: 100%|██████████| 103/103 [02:20<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 102 unique UniProt protein sequences.\n",
      "Total 1652 unique protein sequences (NCBI and UniProt combined).\n",
      "Wrote all sequences to data/full_library_virus_proteins.fasta\n",
      "Wrote sequence metadata to data/full_library_virus_proteins.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Bio import Entrez\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# ---- Parameters ---- #\n",
    "FILE = \"data/VP_library_all_sequences.csv\"\n",
    "TYPE_FILTER = [\"VP\", \"VT\"]\n",
    "ID_COLUMN = \"NCBI_id\"\n",
    "UNIPROT_ID_COLUMN = \"Uniprot_id\"\n",
    "TYPE_COLUMN = \"code\"\n",
    "OUTPUT_FASTA = \"data/full_library_virus_proteins.fasta\"\n",
    "OUTPUT_CSV = \"data/full_library_virus_proteins.csv\"\n",
    "ENTREZ_EMAIL = \"phitro@bu.edu\"  \n",
    "ENTREZ_API_KEY = \"9bb9af72db60905930367f8f543e5ef0d108\"       \n",
    "\n",
    "# UniProt API configuration\n",
    "UNIPROT_BASE_URL = \"https://rest.uniprot.org/uniprotkb/\"\n",
    "UNIPROT_HEADERS = {\n",
    "    \"User-Agent\": f\"PythonScriptForSequenceFetching/1.0 ({ENTREZ_EMAIL})\",\n",
    "    \"Accept\": \"text/fasta\" # Request FASTA format\n",
    "}\n",
    "UNIPROT_RATE_LIMIT_DELAY = 1.0 # UniProt recommends ~1 request per second without a specific key/quota\n",
    "\n",
    "# --- Helper Function: Clean UniProt ID ---\n",
    "def clean_uniprot_id(uniprot_id):\n",
    "    \"\"\"Ensures UniProt ID is canonical (e.g., P05221-1 -> P05221) and handles NaN.\"\"\"\n",
    "    if pd.isna(uniprot_id) or not isinstance(uniprot_id, str):\n",
    "        return \"\"\n",
    "    return uniprot_id.split('-')[0].strip()\n",
    "\n",
    "# ---- Load Data ---- #\n",
    "df = pd.read_csv(FILE)\n",
    "filtered = df[df[TYPE_COLUMN].isin(TYPE_FILTER)].copy()\n",
    "\n",
    "unique_ncbi_ids = sorted(filtered[ID_COLUMN].dropna().astype(str).unique())\n",
    "\n",
    "# Extract unique UniProt IDs, clean them, and flatten if multiple are in one cell\n",
    "flat_uniprot_ids = set()\n",
    "if UNIPROT_ID_COLUMN in filtered.columns:\n",
    "    for ids_str in filtered[UNIPROT_ID_COLUMN].dropna().astype(str).unique():\n",
    "        # Replace common separators (semicolon, comma) with a single comma for splitting\n",
    "        for uniprot_id_raw in ids_str.replace(';', ',').split(','):\n",
    "            cleaned_id = clean_uniprot_id(uniprot_id_raw)\n",
    "            if cleaned_id:\n",
    "                flat_uniprot_ids.add(cleaned_id)\n",
    "unique_uniprot_ids = sorted(list(flat_uniprot_ids))\n",
    "\n",
    "print(f\"Found {len(unique_ncbi_ids)} unique NCBI IDs of type {TYPE_FILTER}\")\n",
    "print(f\"Found {len(unique_uniprot_ids)} unique UniProt IDs to fetch sequences for\")\n",
    "\n",
    "# ---- Set up Entrez ---- #\n",
    "Entrez.email = ENTREZ_EMAIL\n",
    "if ENTREZ_API_KEY:\n",
    "    Entrez.api_key = ENTREZ_API_KEY\n",
    "\n",
    "# ---- Download FASTA sequences (NCBI) ---- #\n",
    "# Use a dictionary to store all sequences, with a key representing (identifier, source) for deduplication\n",
    "all_sequences = {} # Key: (identifier_used_for_header, source), Value: Bio.SeqRecord object\n",
    "seq_df_rows = [] # List to build the DataFrame rows\n",
    "\n",
    "for ncbi_id in tqdm(unique_ncbi_ids, desc=\"Fetching NCBI FASTA\"):\n",
    "    try:\n",
    "        # Try protein database first (most likely)\n",
    "        handle = Entrez.efetch(db=\"protein\", id=ncbi_id, rettype=\"fasta\", retmode=\"text\")\n",
    "        records = list(SeqIO.parse(handle, \"fasta\"))\n",
    "        handle.close()\n",
    "        if records:\n",
    "            for rec in records:\n",
    "                # FASTA header format: NCBI_id|original_rec_id (e.g., 12345|gi|67890|ref|NP_123.1|)\n",
    "                # If multiple records come from one NCBI ID, keep their unique IDs.\n",
    "                # If only one, using just the NCBI_id can be simpler.\n",
    "                header_id_for_fasta = f\"{ncbi_id}|{rec.id}\" if len(records) > 1 else str(ncbi_id)\n",
    "                \n",
    "                seq_content = str(rec.seq)\n",
    "                \n",
    "                # Use a combined key (header_id_for_fasta, source) to ensure uniqueness\n",
    "                if (header_id_for_fasta, \"NCBI\") not in all_sequences:\n",
    "                    rec.id = header_id_for_fasta # Update Bio.SeqRecord ID for the FASTA file\n",
    "                    rec.description = f\"NCBI {ncbi_id} {rec.description}\" # Add context to description\n",
    "                    all_sequences[(header_id_for_fasta, \"NCBI\")] = rec\n",
    "                    seq_df_rows.append({'Identifier': ncbi_id, 'Sequence': seq_content, 'Source': 'NCBI'})\n",
    "        else:\n",
    "            print(f\"NCBI ID {ncbi_id}: No sequences found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for NCBI ID: {ncbi_id}\\nError: {e}\")\n",
    "    time.sleep(0.11)  # NCBI: <=10/sec with API key, <=3/sec without\n",
    "\n",
    "print(f\"Downloaded {len([rec for (id,src),rec in all_sequences.items() if src == 'NCBI'])} unique NCBI protein sequences.\")\n",
    "\n",
    "# ---- Download FASTA sequences (UniProt) ---- #\n",
    "for uniprot_id in tqdm(unique_uniprot_ids, desc=\"Fetching UniProt FASTA\"):\n",
    "    if not uniprot_id: # Skip empty UniProt IDs\n",
    "        continue\n",
    "    try:\n",
    "        response = requests.get(f\"{UNIPROT_BASE_URL}{uniprot_id}.fasta\", headers=UNIPROT_HEADERS, timeout=10)\n",
    "        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "        fasta_content = response.text\n",
    "        records = list(SeqIO.parse(io.StringIO(fasta_content), \"fasta\"))\n",
    "        \n",
    "        if records:\n",
    "            for rec in records:\n",
    "                # UniProt FASTA header from API: >sp|P12345|ABCDE_HUMAN ...\n",
    "                # We want header: UniProt_id_from_DF|original_rec_id (e.g., P12345|sp|P12345|ABCDE_HUMAN)\n",
    "                header_id_for_fasta = f\"{uniprot_id}|{rec.id}\"\n",
    "                \n",
    "                seq_content = str(rec.seq)\n",
    "\n",
    "                # Use a combined key (header_id_for_fasta, source) to ensure uniqueness\n",
    "                if (header_id_for_fasta, \"UniProt\") not in all_sequences:\n",
    "                    rec.id = header_id_for_fasta # Update Bio.SeqRecord ID for the FASTA file\n",
    "                    rec.description = f\"UniProt {uniprot_id} {rec.description}\" # Add context to description\n",
    "                    all_sequences[(header_id_for_fasta, \"UniProt\")] = rec\n",
    "                    seq_df_rows.append({'Identifier': uniprot_id, 'Sequence': seq_content, 'Source': 'UniProt'})\n",
    "        else:\n",
    "            print(f\"UniProt ID {uniprot_id}: No sequences found.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed for UniProt ID: {uniprot_id}\\nError: {e}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            print(f\"  Response status: {e.response.status_code}, content: {e.response.text[:200]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for UniProt ID (unexpected error): {uniprot_id}\\nError: {e}\")\n",
    "    time.sleep(UNIPROT_RATE_LIMIT_DELAY) # Be polite to UniProt API\n",
    "\n",
    "print(f\"Downloaded {len([rec for (id,src),rec in all_sequences.items() if src == 'UniProt'])} unique UniProt protein sequences.\")\n",
    "\n",
    "# ---- Combine all SeqRecords and write to single FASTA file ---- #\n",
    "# Convert dictionary values (SeqRecord objects) to a list\n",
    "all_seq_records_list = [rec for rec in all_sequences.values()] \n",
    "print(f\"Total {len(all_seq_records_list)} unique protein sequences (NCBI and UniProt combined).\")\n",
    "\n",
    "with open(OUTPUT_FASTA, \"w\") as out_handle:\n",
    "    SeqIO.write(all_seq_records_list, out_handle, \"fasta\")\n",
    "\n",
    "# ---- Write sequence metadata to CSV file ---- #\n",
    "seq_df = pd.DataFrame(seq_df_rows)\n",
    "seq_df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"Wrote all sequences to {OUTPUT_FASTA}\")\n",
    "print(f\"Wrote sequence metadata to {OUTPUT_CSV}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_analysis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
